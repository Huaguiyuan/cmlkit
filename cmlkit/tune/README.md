## `cmlkit.tune`

This is the hyperparameter tuning component of `cmlkit`.

It is a wrapper for the `hyperopt` package, embedding it within a custom parallelised asynchronous optimisation infrastructure. 

Important features are:

- Robust parallelisation (on a single computer) with timeouts (i.e. we can
  deal with unstable external components that don't terminate). NO MONGODB
- Resuming/replay: Runs are saved as a series of steps, so they can always be recovered, replayed and continued. The steps are saved as the optimisation proceeds, and writes are atomic, so very little can go wrong.
- Caching of evaluations: no redundant evaluations are undertaken.
- Flexible architecture: Computation of losses, the types of models evaluated, and the stopping criteria can be customised.
- Async architecture: Recognising that model evaluation cannot be expected to take the same time, everything happens asynchronously.
- Transparent, configurable error treatment: Errors can be optionally caught and reported.
- Separation of concerns: As opposed to `hyperopt`, details of the execution are not visible to the optimisation algorithm, the executor doesn't need to know how to compute losses, and so on.

### Architecture

We separate the task of optimising into multiple components:

1. A `Search`, which makes suggestions based on previous results, i.e. the part that you'd colloquially call "optimisation",
2. An `Evaluator` which computes losses (i.e. stores the data needed to compute losses, takes care to train the models, etc.), and
3. A `Run` instance, that takes care of the execution, saving results, etc.

These components communicate with each other in narrowly-defined ways, and avoid conceptual crossover between these distincts concerns. As a result, this architecture is quite general, and easy to extend in well-defined ways.

For users and developers, the most likely extension will be providing custom `Evaluators` to model specific training/prediction procedures, or special losses. Please see below for an overview of the interface you must implement. Also note that it is crucial that `Evaluators` are `Components` -- the communication with parallel workers also happens via the `config` mechanism, so `cmlkit` must know how to instantiate your `Evaluator`!

### Nomenclature

The basic nomenclature is as follows:

- Suggestion: Emitted by the search, it represents one possible model that should be tried. Normally a config, but in general any (serialisable) dict.
- Trial: The task of performing the evaluation of such a suggestion.
- Evaluation: The unique task of evaluating a particular dictionary/config. Since Searches are free to make the same suggestion multiple times, this is not identical to the concept of a Trial. Think of it as the "de-duplicated version" of a Trial.
- tid/eid: Trial ID and Evaluation ID. Trial IDs are generated by the Search and must only occur once during a search, Evaluation IDs are simple the hash of a suggestion.
- tape: The "trajectory" of the optimisation, a series of steps that, if retraced in the same order with the same arguments, yield an identical optimisation state.

Note that we prefer to say "search" as opposed to "optimisation" to reflect the fact that this is fundamentally different from "normal" optimisation methods: There are no guarantees/expectations on the order in which candidate models are evaluated, everything is asynchronous in nature, steps are not particularly sequential.

For implementation details, please read the docstrings of the various components.

### Interfaces

If you implement custom `Evaluators`/`Searches`, here is the interface
that they need to implement.

```
Search:
    suggest() -> tid, suggestion.
        tid must be unique. it identifies this suggestion.
        suggestion must be a dict and should be a valid config.
    submit(tid, error=False, loss=None, var=None).
        tid must match a previously suggested one.
        if the trial failed, error must be true, loss and var are ignored.
        if the trial succeeded, error must be false, loss is required and var is optional.

    Searches must be deterministic with suggest and submit are performed in the same order with the same arguments.

Evaluator (must be a Component):
    __call__(config) -> result.
        result must contain the keys "loss" and "duration".
        it can contain the keys "var" and "refined_config".
        additional keys are ignored.
        exceptions must be raised, not caught.

```

### Caveats

- We assume that evaluations aren't so expensive they need checkpointing, and aren't much faster than about 10s. The latter can be customised to some extent, but there is a builtin assumptions that the event loop of a `Run` doesn't run too fast.
- Currently, re-running from `son` is slow because we use the `yaml` dumper. It is slightly more readable, but at least 10x slower than `json`. This will be changed.

### Roadmap

- `MPI` support. In principle it is easy to extend this architecture to multi-node parallelisation. Preliminary work is already done, but it's currently not a priority.
- `json` instead of `yaml`. Will be done At Some Point.


